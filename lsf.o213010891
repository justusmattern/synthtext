Sender: LSF System <lsfadmin@eu-g3-013>
Subject: Job 213010891: <synthdata> in cluster <euler> Exited

Job <synthdata> was submitted from host <eu-login-22> by user <jmattern> in cluster <euler> at Sun Apr 10 17:15:51 2022
Job was executed on host(s) <20*eu-g3-013>, in queue <gpu.24h>, as user <jmattern> in cluster <euler> at Sun Apr 10 17:15:55 2022
</cluster/home/jmattern> was used as the home directory.
</cluster/work/sachan/jmattern/synthtext> was used as the working directory.
Started at Sun Apr 10 17:15:55 2022
Terminated at Mon Apr 11 17:15:57 2022
Results reported at Mon Apr 11 17:15:57 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python train_classifier.py
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   86404.18 sec.
    Max Memory :                                 5359 MB
    Average Memory :                             4131.43 MB
    Total Requested Memory :                     20480.00 MB
    Delta Memory :                               15121.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                6
    Run time :                                   86401 sec.
    Turnaround time :                            86406 sec.

The output (if any) follows:

Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
User defined signal 2
